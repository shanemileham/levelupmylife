---
title: Eliezer Yudkowsky - Rationality Sequences
---

{% include toc %}

[Download it online](https://intelligence.org/rationality-ai-zombies/)

## Excerpts
### Book I - Map and Territory
#### A. Predictably Wrong
##### 1. [What Do I Mean By “Rationality”?](http://lesswrong.com/lw/31/what_do_we_mean_by_rationality/)
- Epistemic Rationality = systematically improving the accuracy of your beliefs
  - When you open your eyes and look at the room around you, you’ll locate your laptop in relation to the table, and you’ll locate a bookcase in relation to the wall. If something goes wrong with your eyes, or your brain, then your mental model might say there’s a bookcase where no bookcase exists, and when you go over to get a book, you’ll be disappointed. This is what it’s like to have a false belief, a map of the world that doesn’t correspond to the territory. Epistemic rationality is about building accurate maps instead.
- Instrumental Rationality = systematically achieving your values
  - It’s the art of choosing actions that lead to outcomes ranked higher in your preferences. I sometimes call this “winning.”
- Truth = the correspondence between belief and reality
- Probability Theory = the set of laws underlying rational belief
- Bayesian = beliefs and actions that are rational in this mathematically well-defined sense
- So rationality is about forming true beliefs and making winning decisions
- And “winning” here need not come at the expense of others. The project of life can be about collaboration or self-sacrifice, rather than about competition. “Your values” here means anything you care about, including other people. It isn’t restricted to selfish values or unshared values.
- “Rational agents make decisions that maximize the probabilistic expectation of a coherent utility function”
- No one can actually calculate and obey the math, any more than you can predict the stock market by calculating the movements of quarks.

##### 2. [Feeling Rational](http://lesswrong.com/lw/hp/feeling_rational/)
- A popular belief about “rationality” is that rationality opposes all emotion— that all our sadness and all our joy are automatically anti-logical by virtue of being feelings. Yet strangely enough, I can’t find any theorem of probability theory which proves that I should appear ice-cold and expressionless.
- Your beliefs about “how-the-world-is” can concern anything you think is out there in reality, anything that either does or does not exist, any member of the class “things that can make other things happen.” If you believe that there is a goblin in your closet that ties your shoes’ laces together, then this is a belief about how-the-world-is. Your shoes are real— you can pick them up. If there’s something out there that can reach out and tie your shoelaces together, it must be real too, part of the vast web of causes and effects we call the “universe.” Feeling angry at the goblin who tied your shoelaces involves a state of mind that is not just about how-the-world-is. Suppose that, as a Buddhist or a lobotomy patient or just a very phlegmatic person, finding your shoelaces tied together didn’t make you angry. This wouldn’t affect what you expected to see in the world— you’d still expect to open up your closet and find your shoelaces tied together. Your anger or calm shouldn’t affect your best guess here, because what happens in your closet does not depend on your emotional state of mind; though it may take some effort to think that clearly.
- Becoming more rational— arriving at better estimates of how-the-world-is— can diminish feelings or intensify them.
- Ever since I adopted the rule of “That which can be destroyed by the truth should be,” I’ve also come to realize “That which the truth nourishes should thrive.”
- I visualize the past and future of humankind, the tens of billions of deaths over our history, the misery and fear, the search for answers, the trembling hands reaching upward out of so much blood, what we could become someday when we make the stars our cities, all that darkness and all that light—

##### 3. [Why Truth? And...](http://lesswrong.com/lw/go/why_truth_and/)
- Rational Emotion = an emotion that is evoked by correct beliefs or epistemically rational thinking
- I label an emotion as “not rational” if it rests on mistaken beliefs, or rather, on mistake-producing epistemic conduct
- Conversely, then, an emotion that is evoked by correct beliefs or epistemically rational thinking is a “rational emotion”
- When people think of “emotion” and “rationality” as opposed, I suspect that they are really thinking of System 1 and System 2— fast perceptual judgments versus slow deliberative judgments. Deliberative judgments aren’t always true, and perceptual judgments aren’t always false; so it is very important to distinguish that dichotomy from “rationality.” Both systems can serve the goal of truth, or defeat it, depending on how they are used.
- what set humanity firmly on the path of Science was noticing that certain modes of thinking uncovered beliefs that let us manipulate the world.
- Are there motives for seeking truth besides curiosity and pragmatism? The third reason that I can think of is morality: You believe that to seek the truth is noble and important and worthwhile.
- When we write new mental programs for ourselves, they start out in System 2, the deliberate system, and are only slowly— if ever— trained into the neural circuitry that underlies System 1.
- If we want the truth, we can most effectively obtain it by thinking in certain ways, rather than others; these are the techniques of rationality.

##### 4. [What's a Bias, Again?](http://lesswrong.com/lw/gp/whats_a_bias_again/)
- Biases = those obstacles to truth which are produced, not by the cost of information, nor by limited computing power, but by the shape of our own mental machinery.
- Mistakes = errors that arise from cognitive content such as adopted beliefs or moral duties.
- Error is not an exceptional condition; it is success that is a priori so improbable that it requires an explanation.
- You should not ignore something just because you can’t define it. I can’t quote the equations of General Relativity from memory, but nonetheless if I walk off a cliff, I’ll fall. And we can say the same of biases— they won’t hit any less hard if it turns out we can’t define compactly what a “bias” is. So we might point to conjunction fallacies, to overconfidence, to the availability and representativeness heuristics, to base rate neglect, and say: “Stuff like that.”
- Even if we can do no better for knowing, it is still a failure that arises, in an identifiable fashion, from a particular kind of cognitive machinery— not from having too little machinery, but from the machinery’s shape.
- “Biases” are distinguished from errors that arise from cognitive content, such as adopted beliefs, or adopted moral duties. These we call “mistakes,” rather than “biases,” and they are much easier to correct, once we’ve noticed them for ourselves. (Though the source of the mistake, or the source of the source of the mistake, may ultimately be some bias.)
- Personally, I see our quest in terms of acquiring personal skills of rationality, in improving truthfinding technique. The challenge is to attain the positive goal of truth, not to avoid the negative goal of failure. Failurespace is wide, infinite errors in infinite variety. It is difficult to describe so huge a space: “What is true of one apple may not be true of another apple; thus more can be said about a single apple than about all the apples in the world.” Success-space is narrower, and therefore more can be said about it.

##### 5. [Availability](http://lesswrong.com/lw/j5/availability/)
- Availability Heuristic = judging the frequency or probability of an event by the ease with which examples of the event come to mind
- The availability heuristic is judging the frequency or probability of an event by the ease with which examples of the event come to mind.
- Subjects thought that accidents caused about as many deaths as disease; thought that homicide was a more frequent cause of death than suicide. Actually, diseases cause about sixteen times as many deaths as accidents, and suicide is twice as frequent as homicide.
- The skewed probability judgments correlated strongly (0.85 and 0.89) with skewed reporting frequencies in two newspapers. 2 This doesn’t disentangle whether murders are more available to memory because they are more reported-on, or whether newspapers report more on murders because murders are more vivid (hence also more remembered).
- Compared to our ancestors, we live in a larger world, in which far more happens, and far less of it reaches us— a much stronger selection effect, which can create much larger availability biases.
- The objective frequency of Bill Gates is 0.00000000015, but you hear about him much more often.
- when dams and levees are built, they reduce the frequency of floods, and thus apparently create a false sense of security, leading to reduced precautions. 4 While building dams decreases the frequency of floods, damage per flood is afterward so much greater that average yearly damage increases.
- A society well-protected against minor hazards takes no action against major risks, building on flood plains once the regular minor floods are eliminated. A society subject to regular minor hazards treats those minor hazards as an upper bound on the size of the risks, guarding against regular minor floods but not occasional major floods.

##### 6. [Burdensome Details](http://lesswrong.com/lw/jk/burdensome_details/)
- Conjunction Fallacy = when humans rate the probability P(A, B) higher than the probability P(B), even though it is a theorem that P(A, B) ≤ P(B).
- The conjunction fallacy is when humans rate the probability P(A, B) higher than the probability P(B), even though it is a theorem that P(A, B) ≤ P(B).
- conjunction fallacy occurs because we “substitute judgment of representativeness for judgment of probability.”
- By adding extra details, you can make an outcome seem more characteristic of the process that generates it.
- For example, in one experiment in 1981, 68% of the subjects ranked it more likely that “Reagan will provide federal support for unwed mothers and cut federal support to local governments” than that “Reagan will provide federal support for unwed mothers.”
- You can make it sound more plausible that Reagan will support unwed mothers, by adding the claim that Reagan will also cut support to local governments. The implausibility of one claim is compensated by the plausibility of the other; they “average out.”
- Adding detail can make a scenario SOUND MORE PLAUSIBLE, even though the event necessarily BECOMES LESS PROBABLE.
- In the 1982 experiment where professional forecasters assigned systematically higher probabilities to “Russia invades Poland, followed by suspension of diplomatic relations between the USA and the USSR” than to “Suspension of diplomatic relations between the USA and the USSR,” each experimental group was only presented with one proposition.
- What strategy could these forecasters have followed, as a group, that would have eliminated the conjunction fallacy, when no individual knew directly about the comparison?
- It seems to me, that they would need to notice the word “and.” They would need to be wary of it— not just wary, but leap back from it.
- They would need to notice the conjunction of two entire details, and be shocked by the audacity of anyone asking them to endorse such an insanely complicated prediction. And they would need to penalize the probability substantially— a factor of four, at least, according to the experimental details.
- Moreover, they would need to add absurdities— where the absurdity is the log probability, so you can add it— rather than averaging them.
- “Reagan might or might not cut support to local governments (1 bit), but it seems very unlikely that he will support unwed mothers (4 bits). Total absurdity: 5 bits.”
- They would need to feel a stronger emotional impact from Occam’s Razor— feel every added detail as a burden, even a single extra roll of the dice.
- “It is more probable that universes replicate for any reason, than that they replicate via black holes because advanced civilizations manufacture black holes because universes evolve to make them do it.”
- You have to disentangle the details. You have to hold up every one independently, and ask, “How do we know this detail?”

##### 7. [Planning Fallacy]
- Planning fallacy = the phenomena that people cannot plan accurately
- The planning fallacy is that people think they can plan, ha ha.
- Asking subjects for their predictions based on realistic “best guess” scenarios; and Asking subjects for their hoped-for “best case” scenarios . . . . . . produced indistinguishable results. When people are asked for a “realistic” scenario, they envision everything going exactly as planned, with no unexpected delays or unforeseen catastrophes— the same vision as their “best case.” Reality, it turns out, usually delivers results somewhat worse than the “worst case.”
- People tend to generate their predictions by thinking about the particular, unique features of the task at hand, and constructing a scenario for how they intend to complete the task—
- The outside view is when you deliberately avoid thinking about the special, unique features of this project, and just ask how long it took to finish broadly similar projects in the past.
- But experiment has shown that the more detailed subjects’ visualization, the more optimistic (and less accurate) they become.
- A similar finding is that experienced outsiders, who know less of the details, but who have relevant memory to draw upon, are often much less optimistic and much more accurate than the actual planners and implementers. So there is a fairly reliable way to fix the planning fallacy, if you’re doing something broadly similar to a reference class of previous projects. Just ask how long similar projects have taken in the past, without considering any of the special properties of this project. Better yet, ask an experienced outsider how long similar projects have taken.

##### 8. Illusions of Transparency: Why No One Understands You
- In hindsight bias, people who know the outcome of a situation believe the outcome should have been easy to predict in advance. Knowing the outcome, we reinterpret the situation in light of that outcome.
- Closely related is the illusion of transparency: We always know what we mean by our words, and so we expect others to know it too.
- “The goose hangs high” is an archaic English idiom that has passed out of use in modern language. Keysar and Bly told one group of subjects that “the goose hangs high” meant that the future looks good; another group of subjects learned that “the goose hangs high” meant the future looks gloomy. 5 Subjects were then asked which of these two meanings an uninformed listener would be more likely to attribute to the idiom. Each group thought that listeners would perceive the meaning presented as “standard.”

##### 9. Expecting Short Inferential Distances
- In the ancestral environment, you were unlikely to end up more than one inferential step away from anyone else. When you discover a new oasis, you don’t have to explain to your fellow tribe members what an oasis is, or why it’s a good idea to drink water, or how to walk. Only you know where the oasis lies; this is private knowledge. But everyone has the background to understand your description of the oasis, the concepts needed to think about water; this is universal knowledge. When you explain things in an ancestral environment, you almost never have to explain your concepts. At most you have to explain one new concept, not two or more simultaneously.
- In the ancestral environment, anyone who says something with no obvious support is a liar or an idiot.
- Conversely, if you say something blatantly obvious and the other person doesn’t see it, they’re the idiot, or they’re being deliberately obstinate to annoy you.
- A clear argument has to lay out an inferential pathway, starting from what the audience already knows or accepts. If you don’t recurse far enough, you’re just talking to yourself. If at any point you make a statement without obvious justification in arguments you’ve previously supported, the audience just thinks you’re crazy. This also happens when you allow yourself to be seen visibly attaching greater weight to an argument than is justified in the eyes of the audience at that time.

##### 10. The Lens That Sees Its Own Flaws
- we, as humans, can look at a seemingly bizarre image, and realize that part of what we’re seeing is the lens itself.
- The whole idea of Science is, simply, reflective reasoning about a more reliable process for making the contents of your mind mirror the contents of the world.
- But a human brain is a flawed lens that can understand its own flaws— its systematic errors, its biases— and apply second-order corrections to them.

#### B. Fake Beliefs
##### 11. Making Beliefs Pay Rent (in Anticipated Experiences)
- The two think they have different models of the world, but they have no difference with respect to what they expect will happen to them.
- It’s tempting to try to eliminate this mistake class by insisting that the only legitimate kind of belief is an anticipation of sensory experience. But the world does, in fact, contain much that is not sensed directly.
- To answer precisely, you must use beliefs like Earth’s gravity is 9.8 meters per second per second, and This building is around 120 meters tall. These beliefs are not wordless anticipations of a sensory experience; they are verbal-ish, propositional.
- these two beliefs have an inferential consequence that is a direct sensory anticipation
- It is a great strength of Homo sapiens that we can, better than any other species in the world, learn to model the unseen. It is also one of our great weak points. Humans often believe in things that are not only unseen but unreal.
- the link from phlogiston to experience was always configured after the experience, rather than constraining the experience in advance.
- We can build up whole networks of beliefs that are connected only to each other— call these “floating” beliefs. It is a uniquely human flaw among animal species,
- The rationalist virtue of empiricism consists of constantly asking which experiences our beliefs predict— or better yet, prohibit.
- It is even better to ask: what experience must not happen to you?
- A null answer means that your belief does not constrain experience; it permits anything to happen to you. It floats.
- When you argue a seemingly factual question, always keep in mind which difference of anticipation you are arguing about. If you can’t find the difference of anticipation, you’re probably arguing about labels in your belief network— or even worse, floating beliefs, barnacles on your network.
- Above all, don’t ask what to believe— ask what to anticipate. Every question of belief should flow from a question of anticipation, and that question of anticipation should be the center of the inquiry.
- If a belief turns deadbeat, evict it.

##### 12. [A Fable of Science and Politics](http://lesswrong.com/lw/gt/a_fable_of_science_and_politics/)

##### 13. Belief in Belief
- If someone claims there is a dragon in their garage which is invisible, inaudible, does not produce carbon dioxide, is permeable to flour, etc. then…
- The claimant must have an accurate model of the situation somewhere in their mind, because they can anticipate, in advance, exactly which experimental results they’ll need to excuse.
- The claimant does not anticipate anything different
- Belief in belief = believing that it is proper/good/virtuous/beneficial to believe something
- “Belief” should include unspoken anticipation-controllers. “Belief in belief” should include unspoken cognitive-behavior-guiders.
- It is not psychologically realistic to say, “The dragon-claimant does not believe there is a dragon in their garage; they believe it is beneficial to believe there is a dragon in their garage.” But it is realistic to say the dragon-claimant anticipates as if there is no dragon in their garage, and makes excuses as if they believed in the belief.
- And to flinch away from giving up your belief in the dragon— or flinch away from giving up your self-image as a person who believes in the dragon— it is not necessary to explicitly think I want to believe there’s a dragon in my garage. It is only necessary to flinch away from the prospect of admitting you don’t believe.
- To correctly anticipate, in advance, which experimental results shall need to be excused, the dragon-claimant must (a) possess an accurate anticipation-controlling model somewhere in their mind, and (b) act cognitively to protect either (b1) their free-floating propositional belief in the dragon or (b2) their self-image of believing in the dragon.
- When someone makes up excuses in advance, it would seem to require that belief and belief in belief have become unsynchronized.

##### 14. [Bayesian Judo](http://lesswrong.com/lw/i5/bayesian_judo/)


## Compression
### Book I - Map and Territory
#### A. Predictably Wrong
1. Rationality is composed of epistemic rationality (creating accurate beliefs) and instrumental rationality (achieving goals/values); in other words, maximizing your likelihood of winning in whatever sense that means to you.
2. Rationality does not eliminate emotions, although it may increase or decrease them because the state of the world influences us to feel certain ways. However, your emotions do not affect rationality because they should neither alter your model of the world nor the best action to take (except when you can use your emotional state to your advantage, which changes the best action). Two rules of rationality: That which can be destroyed by the truth should be, and That which the truth nourishes should thrive.
3. Three motives to seek truth are curiosity, pragmatism, and morality. Curiosity may not last long and morality may cause trouble, so pragmatism set us on the path of Science to help us manipulate the world. Also a Rational Emotion is an emotion that is evoked by correct beliefs or epistemically rational thinking.
4. A Bias is an obstacle to truth which is produced, not by the cost of information, nor by limited computing power, but by the shape of our own mental machinery. Mistakes, on the other hand, are errors that arise from cognitive content such as adopted beliefs or moral duties. Our goal should be truthfinding instead of error-avoiding; failurespace is wide, but success-space is narrow.
5. The availability heuristic is judging the frequency or probability of an event by the ease with which examples of the event come to mind. In a world that is ever more connected, there exists vastly more information than we can process, so the multi-layered filtering leaves us with information which can create highly skewed availability biases.
6. The conjunction fallacy is when humans rate the probability P(A, B) higher than the probability P(B), even though it is a theorem that P(A, B) ≤ P(B). Adding detail can make a scenario sound more plausible, even though the event necessarily becomes less probable. To counteract the conjunction fallacy, we need to notice the word “and”, intuitively feel that every added detail is a burden, and be shocked by the audacity of anyone asking us to endorse the absurdity of such a conjunction.
7. The planning fallacy is the phenomena that people cannot plan accurately. People tend to generate their predictions by envisioning the known implementation details, not the holes in plans. This leads to people’s “best guess” and “best case” estimates being indistinguishable. To fix this fallacy, relate the length to a known length of a similar endeavor. Better yet, have an experienced outsider make the estimate.
8. The illusion of transparency is the phenomena where we say ambiguous words with an intent of a specific interpretation and we expect others to understand our intent. Spoiler alert: they don’t. More than that, we assume others share our basic contextual understanding for interpreting words or events.
9. Our knowledge is so obvious to us that we expect others to share it. The conclusions we come to are a short inferential distance from what we know, so we expect them to be equally short for others. To overcome this, we need to lay out an inferential pathway from axioms to the conclusion, seeing the information from the other side.
10. As humans, we can think about our thinking and understand the flaws (e.g. systematic errors, biases) and apply second-order corrections to them to more accurately mirror the contents of the world.
#### B. Fake Beliefs
11. x
12. x
13. x
14. x
